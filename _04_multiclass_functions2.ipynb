{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMWOfV+yLJOeAl8Qz6DUIgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hizieun/ToTheTransformer/blob/main/_04_multiclass_functions2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66vVMEeesCte"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np # confusion matrix 사용시\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def Train(model, train_DL, val_DL, criterion, optimizer,\n",
        "          EPOCH, BATCH_SIZE,\n",
        "          save_model_path, save_history_path, **kwargs):\n",
        "\n",
        "    if \"LR_STEP\" in kwargs:\n",
        "        scheduler = StepLR(optimizer, step_size=kwargs[\"LR_STEP\"], gamma=kwargs[\"LR_GAMMA\"])\n",
        "    else:\n",
        "        scheduler = None\n",
        "\n",
        "    loss_history = {\"train\": [], \"val\": []}\n",
        "    acc_history = {\"train\": [], \"val\": []}\n",
        "    best_loss = 9999\n",
        "    for ep in range(EPOCH):\n",
        "        epoch_start = time.time()\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"Epoch: {ep+1}, current_LR = {current_lr}\")\n",
        "\n",
        "        model.train() # train mode로 전환\n",
        "        train_loss, train_acc, _ = loss_epoch(model, train_DL, criterion, optimizer = optimizer)\n",
        "        loss_history[\"train\"] += [train_loss]\n",
        "        acc_history[\"train\"] += [train_acc]\n",
        "        model.eval() # test mode로 전환\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_acc, _ = loss_epoch(model, val_DL, criterion)\n",
        "            loss_history[\"val\"] += [val_loss]\n",
        "            acc_history[\"val\"] += [val_acc]\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                # optimizer도 같이 save하면 여기서부터 재학습 시작 가능\n",
        "                torch.save({\"model\": model,\n",
        "                            \"ep\": ep,\n",
        "                            \"optimizer\": optimizer,\n",
        "                            \"scheduler\": scheduler}, save_model_path)\n",
        "        if \"LR_STEP\" in kwargs:\n",
        "            scheduler.step() # step을 카운트하여\n",
        "        # print loss\n",
        "        print(f\"train loss: {train_loss:.5f}, \"\n",
        "              f\"val loss: {val_loss:.5f} \\n\"\n",
        "              f\"train acc: {train_acc:.1f} %, \"\n",
        "              f\"val acc: {val_acc:.1f} %, time: {time.time()-epoch_start:.0f} s\")\n",
        "        print(\"-\"*20)\n",
        "\n",
        "    torch.save({\"loss_history\": loss_history,\n",
        "                \"acc_history\": acc_history,\n",
        "                \"EPOCH\": EPOCH,\n",
        "                \"BATCH_SIZE\": BATCH_SIZE}, save_history_path)\n",
        "\n",
        "def Test(model,test_DL, criterion):\n",
        "    model.eval() # test mode로 전환\n",
        "    with torch.no_grad():\n",
        "        test_loss, test_acc, rcorrect = loss_epoch(model, test_DL, criterion)\n",
        "    print()\n",
        "    print(f\"Test loss: {test_loss:.3f}\")\n",
        "    print(f\"Test accuracy: {rcorrect}/{len(test_DL.dataset)} ({round(test_acc,1)} %)\")\n",
        "    return round(test_acc,1)\n",
        "\n",
        "def loss_epoch(model, DL, criterion, optimizer = None):\n",
        "    N = len(DL.dataset) # the number of data\n",
        "    rloss = 0; rcorrect = 0\n",
        "    for x_batch, y_batch in tqdm(DL, leave=False): #tqdm(DL, position=10, leave=False): # position은 줄바꿈 개수\n",
        "        x_batch = x_batch.to(DEVICE)\n",
        "        y_batch = y_batch.to(DEVICE)\n",
        "        # inference\n",
        "        y_hat = model(x_batch)\n",
        "        # loss\n",
        "        loss = criterion(y_hat, y_batch)\n",
        "        # update\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad() # gradient 누적을 막기 위한 초기화\n",
        "            loss.backward() # backpropagation\n",
        "            optimizer.step() # weight update\n",
        "        # loss accumulation\n",
        "        loss_b = loss.item() * x_batch.shape[0] # batch loss # BATCH_SIZE 로 하면 마지막 16개도 32개로 계산해버림\n",
        "        rloss += loss_b # running loss\n",
        "        # corrects accumulation\n",
        "        pred = y_hat.argmax(dim=1)\n",
        "        corrects_b = torch.sum(pred == y_batch).item()\n",
        "        rcorrect += corrects_b\n",
        "    loss_e = rloss/N # epoch loss\n",
        "    accuracy_e = rcorrect/N * 100\n",
        "\n",
        "    return loss_e, accuracy_e, rcorrect\n",
        "\n",
        "def Test_plot(model, test_DL):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_batch, y_batch = next(iter(test_DL))\n",
        "        x_batch = x_batch.to(DEVICE)\n",
        "        y_hat = model(x_batch)\n",
        "        pred = y_hat.argmax(dim=1)\n",
        "\n",
        "    x_batch = x_batch.to(\"cpu\")\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    for idx in range(6):\n",
        "        plt.subplot(2,3, idx+1, xticks=[], yticks=[])\n",
        "        plt.imshow(x_batch[idx].permute(1,2,0), cmap=\"gray\")\n",
        "        pred_class = test_DL.dataset.classes[pred[idx]]\n",
        "        true_class = test_DL.dataset.classes[y_batch[idx]]\n",
        "        plt.title(f\"{pred_class} ({true_class})\", color = \"g\" if pred_class==true_class else \"r\")\n",
        "\n",
        "def count_params(model):\n",
        "    num = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    return num"
      ]
    }
  ]
}