{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOcePmFe3vW3i6Uz5R9hGF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hizieun/ToTheTransformer/blob/main/_04_multiclass_functions1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-GMqJaYq2nY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# import numpy as np # confusion matrix 사용시\n",
        "import matplotlib.pyplot as plt\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def Train(model, train_DL, criterion, optimizer, EPOCH):\n",
        "\n",
        "    NoT=len(train_DL.dataset) # Number of training data\n",
        "    loss_history = []\n",
        "\n",
        "    model.train() # train mode로!\n",
        "    for ep in range(EPOCH):\n",
        "        rloss = 0\n",
        "        for x_batch, y_batch in train_DL:\n",
        "            x_batch = x_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "            # inference\n",
        "            y_hat = model(x_batch)\n",
        "            # cross entropy loss\n",
        "            loss = criterion(y_hat, y_batch)\n",
        "            # update\n",
        "            optimizer.zero_grad() # gradient 누적을 막기 위한 초기화\n",
        "            loss.backward() # backpropagation\n",
        "            optimizer.step() # weight update\n",
        "            # loss accumulation\n",
        "            loss_b = loss.item() * x_batch.shape[0] # batch loss # BATCH_SIZE 로 하면 마지막 16개도 32개로 계산해버림\n",
        "            rloss += loss_b # running loss\n",
        "        # print loss\n",
        "        loss_e = rloss/NoT # epoch loss\n",
        "        loss_history += [loss_e]\n",
        "        print(f\"Epoch: {ep+1} train loss: {round(loss_e,3)}\")\n",
        "        print(\"-\"*20)\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "def Test(model,test_DL):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        rcorrect = 0\n",
        "        # rloss = 0\n",
        "        for x_batch, y_batch in test_DL:\n",
        "            x_batch = x_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "            # inference\n",
        "            y_hat = model(x_batch)\n",
        "            # loss\n",
        "            # loss = criterion(y_hat, y_batch) # 만약 쓰려면 매개변수에 criterion 추가\n",
        "            # loss accumulation\n",
        "            # loss_b = loss.item() * x_batch.shape[0] # BATCH_SIZE 로 하면 마지막 16개도 32개로 계산해버림\n",
        "            # rloss += loss_b # running loss\n",
        "            # corrects accumulation\n",
        "            pred = y_hat.argmax(dim=1)\n",
        "            corrects_b = torch.sum(pred == y_batch).item()\n",
        "            rcorrect += corrects_b\n",
        "        # loss_e = rloss/NoTes\n",
        "        accuracy_e = rcorrect/len(test_DL.dataset)*100\n",
        "    # print(\"test loss: \", round(loss_e,2))\n",
        "    print(f\"Test accuracy: {rcorrect}/{len(test_DL.dataset)} ({round(accuracy_e,1)} %)\")\n",
        "    return round(accuracy_e,1)\n",
        "\n",
        "def Test_plot(model, test_DL):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_batch, y_batch = next(iter(test_DL))\n",
        "        x_batch = x_batch.to(DEVICE)\n",
        "        y_hat = model(x_batch)\n",
        "        pred = y_hat.argmax(dim=1)\n",
        "\n",
        "    x_batch = x_batch.to(\"cpu\")\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    for idx in range(6):\n",
        "        plt.subplot(2,3, idx+1, xticks=[], yticks=[])\n",
        "        plt.imshow(x_batch[idx].permute(1,2,0), cmap=\"gray\")\n",
        "        pred_class = test_DL.dataset.classes[pred[idx]]\n",
        "        true_class = test_DL.dataset.classes[y_batch[idx]]\n",
        "        plt.title(f\"{pred_class} ({true_class})\", color = \"g\" if pred_class==true_class else \"r\")\n",
        "\n",
        "def count_params(model):\n",
        "    num = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "    return num\n"
      ]
    }
  ]
}